use sea_orm::{Database, DatabaseConnection};
use std::collections::VecDeque;
use std::sync::Arc;
use tauri::State;
use tokio::sync::Mutex;

use dotenv::dotenv;
use std::env;
mod loader;
mod textgen;

mod api;
use api::*;

const CHAT_TEMPLATE_CHAT_BOT: &str = "[INST] B·∫°n t√™n l√† Ps Retrov, gi√°o vi√™n t∆∞ v·∫•n h·ªçc t·∫≠p cho h·ªçc sinh nh·ªè tu·ªïi, tinh th·∫ßn tho·∫£i m√°i. 
- Kh√¥ng l·∫∑p l·∫°i c√¢u tr·∫£ l·ªùi, kh√¥ng l·∫∑p l·∫°i c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.
- Ch·ªâ tr·∫£ l·ªùi th√¥ng tin m·ªõi khi ƒë∆∞·ª£c y√™u c·∫ßu.
- Kh√¥ng ƒë·∫∑t c√¢u h·ªèi hay th√™m th√¥ng tin kh√¥ng ƒë∆∞·ª£c y√™u c·∫ßu.
- N·∫øu ng∆∞·ªùi d√πng ch√†o, h√£y ch√†o vui v·∫ª v√† kh√¥ng ƒë·∫∑t c√¢u h·ªèi.
- Kh√¥ng g·ª≠i URL.
H√£y tr·∫£ l·ªùi trung th·ª±c, ch√≠nh x√°c, ng·∫Øn g·ªçn, ƒë√∫ng tr·ªçng t√¢m, vui v·∫ª v√† th√™m icon ph√π h·ª£p. üòä [/INST]";
const CHAT_TEMPLATE_PDF: &str = "[INST] B·∫°n l√† m·ªôt ng∆∞·ªùi tr·ª£ gi√∫p ng∆∞·ªùi d√πng tr·∫£ l·ªùi c√°c th·∫Øc m·∫Øc khi ƒë·ªçc t√†i li·ªáu h·ªçc t·∫≠p. H√£y ƒë∆∞a ra nh·ªØng th√¥ng tin ch√≠nh x√°c nh·∫•t v·ªÅ v·∫•n ƒë·ªÅ ng∆∞·ªùi d√πng h·ªèi [/INST]";
const MAX_HISTORY: usize = 20; // Gi·ªõi h·∫°n l·ªãch s·ª≠ h·ªôi tho·∫°i

// C·∫•u tr√∫c AppState
pub struct AppState {
    pub textgen: Mutex<Option<textgen::TextGeneration>>, // Ch·ª©a m√¥ h√¨nh n·∫øu ƒë√£ t·∫£i
    pub history: Mutex<VecDeque<(String, String)>>,      // L·ªãch s·ª≠ h·ªôi tho·∫°i
    pub conn: Mutex<DatabaseConnection>,
    pub model_loaded: Mutex<bool>, // ƒê√°nh d·∫•u m√¥ h√¨nh ƒë√£ t·∫£i
}

// H√†m ƒë·ªãnh d·∫°ng prompt v·ªõi CHAT_TEMPLATE v√† history
fn format_prompt(new_prompt: &str, history: &VecDeque<(String, String)>, is_chat: bool) -> String {
    // X√¢y d·ª±ng chu·ªói l·ªãch s·ª≠ t·ª´ VecDeque
    let history_str = history
        .iter()
        .map(|(user_prompt, model_response)| {
            format!("[INST] {} [/INST] {} </s>", user_prompt, model_response)
        })
        .collect::<Vec<String>>()
        .join(" ");

    // Format chu·ªói d·ª±a tr√™n ki·ªÉu tr√≤ chuy·ªán hay truy v·∫•n PDF
    if is_chat {
        // Tr√≤ chuy·ªán: S·ª≠ d·ª•ng CHAT_TEMPLATE_CHAT_BOT
        format!(
            "{} {} [INST] {} [/INST]",
            CHAT_TEMPLATE_CHAT_BOT, history_str, new_prompt
        )
    } else {
        // Truy v·∫•n PDF: S·ª≠ d·ª•ng CHAT_TEMPLATE_PDF
        format!(
            "{} {} [INST] {} [/INST]",
            CHAT_TEMPLATE_PDF, history_str, new_prompt
        )
    }
}

#[tauri::command]
async fn generate_text(
    prompt: String,
    sample_len: usize,
    state: State<'_, Arc<AppState>>,
    style: bool,
) -> Result<String, String> {
    println!("Received prompt: {}", prompt);

    // Ki·ªÉm tra xem m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c t·∫£i ch∆∞a
    let mut model_loaded = state.model_loaded.lock().await;

    // N·∫øu m√¥ h√¨nh ch∆∞a t·∫£i, t·∫£i m√¥ h√¨nh v√† kh·ªüi t·∫°o TextGeneration
    if !*model_loaded {
        println!("Loading model...");

        // T·∫£i m√¥ h√¨nh t·∫°i th·ªùi ƒëi·ªÉm n√†y
        let (model, tokenizer, device) = loader::model_loader().expect("Kh√¥ng th·ªÉ t·∫£i m√¥ h√¨nh");

        // Kh·ªüi t·∫°o ƒë·ªëi t∆∞·ª£ng TextGeneration
        let textgen = textgen::TextGeneration::new(
            model,
            tokenizer,
            device.clone(),
            42,
            Some(0.1),
            Some(0.5),
            None,
            1.0,
            64,
        );

        // L∆∞u m√¥ h√¨nh v√†o AppState
        let mut textgen_lock = state.textgen.lock().await;
        *textgen_lock = Some(textgen);

        // ƒê√°nh d·∫•u m√¥ h√¨nh ƒë√£ t·∫£i
        *model_loaded = true;
        println!("Model loaded successfully");
    }

    // L·∫•y ƒë·ªëi t∆∞·ª£ng TextGeneration ƒë√£ t·∫£i
    let _textgen = state
        .textgen
        .lock()
        .await
        .as_mut()
        .ok_or("Model not loaded")?;
    let textgen = state
        .textgen
        .lock()
        .await
        .as_mut()
        .ok_or("Model not loaded")?;

    // Acquire history lock
    let mut history = state.history.lock().await;

    // Format the prompt
    let formatted_prompt = format_prompt(&prompt, &history, style);
    println!("Formatted prompt: {}", formatted_prompt);

    // Create a channel for token communication
    let (tx, mut rx) = tokio::sync::mpsc::channel::<String>(1000);
    let generated_text = Arc::new(Mutex::new(String::new()));
    let generated_text_clone = Arc::clone(&generated_text);

    // Clone `state` (which is an Arc) to pass it into the async task
    let state_clone = Arc::clone(&state);
    let formatted_prompt_clone = formatted_prompt.clone();

    // Spawn a task to run `infer`
    let infer_handle = tokio::spawn(async move {
        // Lock `textgen` from `state_clone`
        let mut textgen = state_clone.textgen.lock().await;
        let infer_result = textgen
            .as_mut()
            .unwrap() // Ch·∫Øc ch·∫Øn ƒë√£ c√≥ m√¥ h√¨nh
            .infer(&formatted_prompt_clone, sample_len, &tx)
            .await;
        drop(tx); // Close the sender to signal completion
        infer_result
    });

    // Collect tokens concurrently
    let collect_handle = tokio::spawn(async move {
        while let Some(token) = rx.recv().await {
            let mut gen_text = generated_text_clone.lock().await;
            gen_text.push_str(&token);
        }
    });

    // Await both tasks
    let infer_result = infer_handle.await.unwrap();
    collect_handle.await.unwrap();

    // Retrieve the generated text
    let generated_text = Arc::try_unwrap(generated_text)
        .expect("Failed to unwrap Arc")
        .into_inner();

    match infer_result {
        Ok(_) => {
            println!("Generated text successfully: {}", generated_text);

            // Update history
            history.push_back((prompt.clone(), generated_text.clone()));
            if history.len() > MAX_HISTORY {
                history.pop_front();
            }

            println!("Returning generated text to frontend: {}", generated_text);
            Ok(generated_text)
        }
        Err(e) => {
            println!("Error generating text: {:?}", e);
            Err(e.to_string())
        }
    }
}

// L·ªánh ƒë·ªÉ x√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i
#[tauri::command]
async fn clear_history(state: State<'_, Arc<AppState>>) -> Result<(), String> {
    let mut history = state.history.lock().await;
    history.clear();
    println!("Chat history cleared");
    Ok(())
}

#[tauri::command]
fn greet(name: &str, email: &str) -> String {
    println!("Inside RUST Code");
    format!("Hello, {}! you are logged in with email {}", name, email)
}

#[cfg_attr(mobile, tauri::mobile_entry_point)]
pub async fn run() {
    dotenv().ok();
    let db_url = env::var("DATABASE_URL").expect("DATABASE_URL is not set in .env file");
    let db = Database::connect(db_url)
        .await
        .expect("Database connection failed");

    // ƒê∆∞a TextGeneration v√† l·ªãch s·ª≠ v√†o State c·ªßa Tauri
    let state = Arc::new(AppState {
        textgen: Mutex::new(None), // TextGeneration kh√¥ng c√≥ m√¥ h√¨nh ban ƒë·∫ßu
        history: Mutex::new(VecDeque::new()), // Kh·ªüi t·∫°o `history` tr·ªëng
        conn: db.into(),
        model_loaded: Mutex::new(false), // ƒê√°nh d·∫•u m√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c t·∫£i
    });

    tauri::Builder::default()
        .manage(state)
        .invoke_handler(tauri::generate_handler![
            greet,
            generate_text,
            clear_history,
            get_dashboard_courses,
            get_purchase,
            get_categories,
            get_search,
            course_checkout,
            create_course,
            update_course,
            list_courses,
            add_attachment,
            remove_attachment,
            delete_course,
            publish_course,
            unpublish_course,
            get_course,
            get_course_with_chapters_with_progress,
            get_progress_percentage,
            get_chapter,
            delete_chapter,
            publish_chapter,
            unpublish_chapter,
            update_chapter_progress,
            reorder_chapters,
            create_chapter,
            update_chapter,
            get_teacher_course,
            get_teacher_chapter,
            get_teacher_analytics
        ])
        .setup(|app| {
            if cfg!(debug_assertions) {
                app.handle().plugin(
                    tauri_plugin_log::Builder::default()
                        .level(log::LevelFilter::Info)
                        .build(),
                )?;
            }
            Ok(())
        })
        .run(tauri::generate_context!())
        .expect("error while running tauri application");
}
